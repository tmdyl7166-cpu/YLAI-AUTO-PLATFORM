#!/bin/bash
# å¤šæ¨¡å‹é¢„å®‰è£…è„šæœ¬
# è”åŠ¨éƒ¨ç½²å››ä¸ªæœ¬åœ°AIæ¨¡å‹ï¼Œå®ç°ä¸“ç”¨æ€§åŠŸèƒ½åˆ†é…

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ£€æŸ¥dockeræ˜¯å¦è¿è¡Œ
check_docker() {
    if ! docker info >/dev/null 2>&1; then
        log_error "Dockeræœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨Docker"
        exit 1
    fi
}

# ç­‰å¾…æœåŠ¡å¥åº·æ£€æŸ¥
wait_for_service() {
    local service_name=$1
    local max_attempts=30
    local attempt=1

    log_info "ç­‰å¾… $service_name æœåŠ¡å¯åŠ¨..."

    while [ $attempt -le $max_attempts ]; do
        if docker ps | grep -q "$service_name" && docker exec "$service_name" ollama list >/dev/null 2>&1; then
            log_success "$service_name æœåŠ¡å·²å°±ç»ª"
            return 0
        fi

        log_info "ç­‰å¾… $service_name... ($attempt/$max_attempts)"
        sleep 10
        ((attempt++))
    done

    log_error "$service_name æœåŠ¡å¯åŠ¨å¤±è´¥"
    return 1
}

# æ‹‰å–å•ä¸ªæ¨¡å‹
pull_model() {
    local service_name=$1
    local model_name=$2
    local display_name=$3

    log_info "å¼€å§‹æ‹‰å– $display_name ($model_name)..."

    if docker exec "$service_name" ollama list | grep -q "$model_name"; then
        log_success "$display_name å·²å­˜åœ¨ï¼Œè·³è¿‡æ‹‰å–"
        return 0
    fi

    log_info "æ‹‰å– $display_name ä¸­..."
    if docker exec "$service_name" ollama pull "$model_name"; then
        log_success "$display_name æ‹‰å–æˆåŠŸ"
        return 0
    else
        log_error "$display_name æ‹‰å–å¤±è´¥"
        return 1
    fi
}

# ä¸»å‡½æ•°
main() {
    log_info "=== YLAI å¤šæ¨¡å‹AIè”åŠ¨éƒ¨ç½²é¢„å®‰è£…è„šæœ¬ ==="
    log_info "åŠŸèƒ½åˆ†é…ï¼š"
    log_info "  1. qwen3:8b     - ä¸­æ–‡å¤„ç†ä¸å†…å®¹ç†è§£"
    log_info "  2. llama3.1:8b  - ä»»åŠ¡è§„åˆ’ä¸æŒ‡ä»¤ç†è§£"
    log_info "  3. deepseek-r1:8b - å¤æ‚æ¨ç†ä¸å†³ç­–åˆ¶å®š"
    log_info "  4. gpt-oss:20b  - åˆ›æ„ç”Ÿæˆä¸æ–‡æœ¬ä¼˜åŒ–"
    echo

    check_docker

    # å¯åŠ¨æ‰€æœ‰OllamaæœåŠ¡
    log_info "å¯åŠ¨æ‰€æœ‰OllamaæœåŠ¡..."
    docker compose -f docker-compose.multi.yml up -d ollama-qwen ollama-llama ollama-deepseek ollama-gptoss

    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    wait_for_service "ylai-ollama-qwen" || exit 1
    wait_for_service "ylai-ollama-llama" || exit 1
    wait_for_service "ylai-ollama-deepseek" || exit 1
    wait_for_service "ylai-ollama-gptoss" || exit 1

    echo
    log_info "å¼€å§‹æ‹‰å–AIæ¨¡å‹..."

    # æŒ‰é¡ºåºæ‹‰å–æ¨¡å‹ï¼ˆä»å°åˆ°å¤§ï¼Œä¾¿äºç›‘æ§è¿›åº¦ï¼‰
    local pull_order=(
        "ylai-ollama-qwen:qwen3:8b:Qwen3 (ä¸­æ–‡å¤„ç†)"
        "ylai-ollama-llama:llama3.1:8b:Llama3.1 (ä»»åŠ¡è§„åˆ’)"
        "ylai-ollama-deepseek:deepseek-r1:8b:DeepSeek-R1 (å¤æ‚æ¨ç†)"
        "ylai-ollama-gptoss:gpt-oss:20b:GPT-OSS (åˆ›æ„ç”Ÿæˆ)"
    )

    local failed_models=()

    for model_spec in "${pull_order[@]}"; do
        IFS=':' read -r service model display_name <<< "$model_spec"

        if pull_model "$service" "$model" "$display_name"; then
            log_success "$display_name éƒ¨ç½²æˆåŠŸ"
        else
            log_error "$display_name éƒ¨ç½²å¤±è´¥"
            failed_models+=("$display_name")
        fi
        echo
    done

    # æ£€æŸ¥ç»“æœ
    if [ ${#failed_models[@]} -eq 0 ]; then
        log_success "=== æ‰€æœ‰AIæ¨¡å‹éƒ¨ç½²æˆåŠŸï¼ ==="
        echo
        log_info "æ¨¡å‹åŠŸèƒ½è”åŠ¨è¯´æ˜ï¼š"
        log_info "  ğŸ“ å†…å®¹ç†è§£ â†’ qwen3:8b (ä¸­æ–‡æ–‡æ¡£åˆ†æ)"
        log_info "  ğŸ§  ä»»åŠ¡è§„åˆ’ â†’ llama3.1:8b (æŒ‡ä»¤ç†è§£åè°ƒ)"
        log_info "  ğŸ¤” å¤æ‚æ¨ç† â†’ deepseek-r1:8b (ç­–ç•¥å†³ç­–ä¼˜åŒ–)"
        log_info "  ğŸ¨ åˆ›æ„ç”Ÿæˆ â†’ gpt-oss:20b (æ–‡æœ¬æ¶¦è‰²å¢å¼º)"
        echo
        log_info "å¯åŠ¨å®Œæ•´æœåŠ¡ï¼š"
        log_info "  docker compose -f docker-compose.multi.yml up -d"
        echo
        log_info "è®¿é—®åœ°å€ï¼š"
        log_info "  å‰ç«¯ç•Œé¢: http://localhost:9000"
        log_info "  AIç³»ç»Ÿ:   http://localhost:9001"
    else
        log_error "=== éƒ¨åˆ†æ¨¡å‹éƒ¨ç½²å¤±è´¥ ==="
        log_error "å¤±è´¥çš„æ¨¡å‹: ${failed_models[*]}"
        log_info "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œç£ç›˜ç©ºé—´åé‡è¯•"
        exit 1
    fi
}

# å‚æ•°å¤„ç†
case "${1:-}" in
    "status")
        log_info "æ£€æŸ¥æ¨¡å‹çŠ¶æ€..."
        docker compose -f docker-compose.multi.yml ps
        echo
        log_info "OllamaæœåŠ¡çŠ¶æ€:"
        for service in qwen llama deepseek gptoss; do
            echo -n "  $service: "
            if docker exec "ylai-ollama-$service" ollama list >/dev/null 2>&1; then
                echo "è¿è¡Œä¸­"
            else
                echo "æœªè¿è¡Œ"
            fi
        done
        ;;
    "clean")
        log_warning "æ¸…ç†æ‰€æœ‰AIæ¨¡å‹æ•°æ®..."
        docker compose -f docker-compose.multi.yml down -v
        log_success "æ¸…ç†å®Œæˆ"
        ;;
    *)
        main "$@"
        ;;
esac