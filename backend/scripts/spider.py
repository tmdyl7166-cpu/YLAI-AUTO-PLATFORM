"""
âœ… AIå¢å¼ºçˆ¬è™«è„šæœ¬
åŠŸèƒ½ï¼šé›†æˆAIæ¨¡å‹çš„æ™ºèƒ½ç½‘é¡µæ•°æ®çˆ¬å–ä¸é€†å‘æ¨ç†
"""
import asyncio
import aiohttp
import json
import re
import hashlib
from typing import Dict, Any, List, Optional, Union
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import time
import random

from backend.core.base import BaseScript
from backend.core.registry import registry
from backend.core.logger import logger
from backend.scripts.ai_coordinator import AIModelCoordinator


@registry.register("spider")
class SpiderScript(BaseScript):
    """AIå¢å¼ºç½‘é¡µçˆ¬è™«è„šæœ¬"""

    name = "spider"
    description = "AIå¢å¼ºæ™ºèƒ½çˆ¬è™«"
    version = "2.0.0"

    def __init__(self):
        super().__init__()
        self.ai_coordinator = None
        self.session = None
        self.visited_urls = set()
        self.max_depth = 3
        self.max_pages = 50
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]

    async def pre_run(self, **kwargs):
        """åˆå§‹åŒ–AIåè°ƒå™¨å’ŒHTTPä¼šè¯"""
        await super().pre_run(**kwargs)

        # åˆå§‹åŒ–AIåè°ƒå™¨
        try:
            self.ai_coordinator = AIModelCoordinator()
            await self.ai_coordinator.initialize()
            logger.info("âœ… AIåè°ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ AIåè°ƒå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.ai_coordinator = None

        # åˆå§‹åŒ–HTTPä¼šè¯
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(timeout=timeout)

    async def run(self, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡ŒAIå¢å¼ºçˆ¬è™«ä»»åŠ¡
        å‚æ•°:
            url (str): ç›®æ ‡ç½‘å€
            depth (int): çˆ¬å–æ·±åº¦ï¼Œé»˜è®¤3
            max_pages (int): æœ€å¤§é¡µé¢æ•°ï¼Œé»˜è®¤50
            ai_enhanced (bool): æ˜¯å¦å¯ç”¨AIå¢å¼ºï¼Œé»˜è®¤True
            target_data (str): ç›®æ ‡æ•°æ®ç±»å‹ ('content', 'structure', 'api', 'auto')
        è¿”å›:
            dict: åŒ…å«çˆ¬å–ç»“æœå’ŒAIåˆ†æ
        """
        url = kwargs.get("url", "https://example.com")
        depth = kwargs.get("depth", self.max_depth)
        max_pages = kwargs.get("max_pages", self.max_pages)
        ai_enhanced = kwargs.get("ai_enhanced", True)
        target_data = kwargs.get("target_data", "auto")

        logger.info(f"ğŸ•·ï¸ğŸ¤– AIå¢å¼ºçˆ¬è™«å¯åŠ¨ - ç›®æ ‡: {url}")
        logger.info(f"ğŸ¯ ç›®æ ‡æ•°æ®ç±»å‹: {target_data}, AIå¢å¼º: {ai_enhanced}")

        try:
            results = {
                "status": "success",
                "url": url,
                "pages_crawled": 0,
                "data_collected": [],
                "ai_analysis": {},
                "reverse_engineering": {},
                "recommendations": []
            }

            # AIé¢„åˆ†æï¼šç†è§£ç›®æ ‡ç½‘ç«™ç»“æ„
            if ai_enhanced and self.ai_coordinator:
                pre_analysis = await self._ai_pre_analysis(url, target_data)
                results["ai_analysis"]["pre_analysis"] = pre_analysis

                # æ ¹æ®AIåˆ†æè°ƒæ•´çˆ¬å–ç­–ç•¥
                if pre_analysis.get("suggested_strategy"):
                    depth = pre_analysis["suggested_strategy"].get("depth", depth)
                    max_pages = pre_analysis["suggested_strategy"].get("max_pages", max_pages)
                    logger.info(f"ğŸ“Š AIè°ƒæ•´ç­–ç•¥ - æ·±åº¦:{depth}, é¡µé¢:{max_pages}")

            # æ‰§è¡Œæ™ºèƒ½çˆ¬å–
            crawl_results = await self._intelligent_crawl(url, depth, max_pages, target_data)
            results.update(crawl_results)

            # AIååˆ†æï¼šæ•°æ®å¤„ç†å’Œé€†å‘æ¨ç†
            if ai_enhanced and self.ai_coordinator and results["data_collected"]:
                post_analysis = await self._ai_post_analysis(results["data_collected"], target_data)
                results["ai_analysis"]["post_analysis"] = post_analysis

                # é€†å‘å·¥ç¨‹åˆ†æ
                reverse_analysis = await self._reverse_engineering_analysis(results["data_collected"])
                results["reverse_engineering"] = reverse_analysis

                # ç”Ÿæˆä¼˜åŒ–å»ºè®®
                recommendations = await self._generate_recommendations(results)
                results["recommendations"] = recommendations

            logger.info(f"âœ… çˆ¬å–å®Œæˆ - é¡µé¢:{results['pages_crawled']}, æ•°æ®é¡¹:{len(results['data_collected'])}")
            return results

        except Exception as e:
            logger.error(f"âŒ çˆ¬è™«æ‰§è¡Œå¤±è´¥: {e}")
            return {"status": "failed", "error": str(e)}

    async def _ai_pre_analysis(self, url: str, target_data: str) -> Dict[str, Any]:
        """AIé¢„åˆ†æï¼šç†è§£ç½‘ç«™ç»“æ„å’Œåˆ¶å®šçˆ¬å–ç­–ç•¥"""
        try:
            prompt = f"""
            åˆ†æç›®æ ‡ç½‘ç«™: {url}
            ç›®æ ‡æ•°æ®ç±»å‹: {target_data}

            è¯·æä¾›ä»¥ä¸‹åˆ†æ:
            1. ç½‘ç«™ç±»å‹å’Œä¸»è¦å†…å®¹
            2. å»ºè®®çš„çˆ¬å–ç­–ç•¥ï¼ˆæ·±åº¦ã€é¡µé¢æ•°ã€ä¼˜å…ˆçº§ï¼‰
            3. æ½œåœ¨çš„æ•°æ®ç»“æ„å’ŒAPIç«¯ç‚¹
            4. åçˆ¬è™«æœºåˆ¶è¯†åˆ«
            5. ä¼˜åŒ–å»ºè®®

            è¿”å›JSONæ ¼å¼ã€‚
            """

            analysis_result = await self.ai_coordinator.run('analyze_content', content=prompt)

            if analysis_result.get('status') == 'success':
                return analysis_result.get('result', {})
            else:
                logger.warning("AIé¢„åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥")
                return {
                    "suggested_strategy": {"depth": 2, "max_pages": 20},
                    "content_type": "unknown",
                    "anti_crawler": "basic"
                }

        except Exception as e:
            logger.error(f"AIé¢„åˆ†æå¼‚å¸¸: {e}")
            return {}

    async def _intelligent_crawl(self, start_url: str, depth: int, max_pages: int, target_data: str) -> Dict[str, Any]:
        """æ™ºèƒ½çˆ¬å–ç®—æ³•"""
        results = {
            "pages_crawled": 0,
            "data_collected": [],
            "structure_analysis": {},
            "api_endpoints": []
        }

        queue = [(start_url, 0)]  # (url, depth)
        visited = set()

        while queue and results["pages_crawled"] < max_pages:
            current_url, current_depth = queue.pop(0)

            if current_url in visited or current_depth > depth:
                continue

            visited.add(current_url)
            results["pages_crawled"] += 1

            try:
                # æ™ºèƒ½è¯·æ±‚å¤´é€‰æ‹©
                headers = {
                    'User-Agent': random.choice(self.user_agents),
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1'
                }

                # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¢«æ£€æµ‹
                await asyncio.sleep(random.uniform(1, 3))

                async with self.session.get(current_url, headers=headers) as response:
                    if response.status != 200:
                        logger.warning(f"é¡µé¢è¯·æ±‚å¤±è´¥ {current_url}: {response.status}")
                        continue

                    content = await response.text()
                    content_type = response.headers.get('content-type', '')

                    # æ ¹æ®ç›®æ ‡æ•°æ®ç±»å‹å¤„ç†å†…å®¹
                    page_data = await self._process_page_content(
                        current_url, content, content_type, target_data
                    )

                    if page_data:
                        results["data_collected"].append(page_data)

                    # æ™ºèƒ½é“¾æ¥æå–å’Œè¿‡æ»¤
                    if current_depth < depth:
                        links = await self._extract_links_smart(content, current_url, target_data)
                        for link in links:
                            if link not in visited:
                                queue.append((link, current_depth + 1))

            except Exception as e:
                logger.error(f"çˆ¬å–é¡µé¢å¤±è´¥ {current_url}: {e}")
                continue

        return results

    async def _process_page_content(self, url: str, content: str, content_type: str, target_data: str) -> Optional[Dict[str, Any]]:
        """æ™ºèƒ½å†…å®¹å¤„ç†"""
        try:
            page_data = {
                "url": url,
                "content_type": content_type,
                "timestamp": time.time(),
                "size": len(content)
            }

            if 'text/html' in content_type:
                # HTMLå†…å®¹å¤„ç†
                soup = BeautifulSoup(content, 'html.parser')

                # æå–æ ‡é¢˜
                title = soup.title.string if soup.title else ""
                page_data["title"] = title.strip() if title else ""

                # æå–ä¸»è¦å†…å®¹
                main_content = self._extract_main_content(soup)
                page_data["content"] = main_content

                # æå–å…ƒæ•°æ®
                meta_data = self._extract_meta_data(soup)
                page_data["meta"] = meta_data

                # æå–ç»“æ„åŒ–æ•°æ®
                structured_data = self._extract_structured_data(soup)
                page_data["structured_data"] = structured_data

            elif 'application/json' in content_type:
                # JSON APIå“åº”
                try:
                    json_data = json.loads(content)
                    page_data["json_data"] = json_data
                    page_data["api_type"] = "json"
                except:
                    page_data["raw_content"] = content[:1000]  # é™åˆ¶å¤§å°

            else:
                # å…¶ä»–ç±»å‹å†…å®¹
                page_data["raw_content"] = content[:2000] if len(content) > 2000 else content

            # æ ¹æ®ç›®æ ‡æ•°æ®ç±»å‹æ·»åŠ ç‰¹å®šå¤„ç†
            if target_data == "content":
                page_data["processed_content"] = await self._process_text_content(page_data)
            elif target_data == "structure":
                page_data["structure_analysis"] = await self._analyze_page_structure(page_data)
            elif target_data == "api":
                page_data["api_analysis"] = await self._analyze_api_structure(page_data)

            return page_data

        except Exception as e:
            logger.error(f"å†…å®¹å¤„ç†å¤±è´¥ {url}: {e}")
            return None

    async def _extract_links_smart(self, content: str, base_url: str, target_data: str) -> List[str]:
        """æ™ºèƒ½é“¾æ¥æå–"""
        try:
            soup = BeautifulSoup(content, 'html.parser')
            links = []

            # æå–æ‰€æœ‰é“¾æ¥
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                absolute_url = urljoin(base_url, href)

                # è¿‡æ»¤æ¡ä»¶
                parsed = urlparse(absolute_url)
                if (parsed.scheme in ['http', 'https'] and
                    parsed.netloc and
                    absolute_url not in self.visited_urls):

                    # æ ¹æ®ç›®æ ‡æ•°æ®ç±»å‹è¿‡æ»¤é“¾æ¥
                    if target_data == "content" and self._is_content_page(absolute_url, a_tag):
                        links.append(absolute_url)
                    elif target_data == "api" and self._is_api_endpoint(absolute_url):
                        links.append(absolute_url)
                    elif target_data == "auto":
                        links.append(absolute_url)

            # é™åˆ¶é“¾æ¥æ•°é‡ï¼Œé¿å…è¿‡åº¦çˆ¬å–
            return links[:10]  # æ¯ä¸ªé¡µé¢æœ€å¤š10ä¸ªé“¾æ¥

        except Exception as e:
            logger.error(f"é“¾æ¥æå–å¤±è´¥: {e}")
            return []

    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """æå–ä¸»è¦å†…å®¹"""
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for script in soup(["script", "style"]):
            script.decompose()

        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        content_selectors = [
            'main', 'article', '.content', '#content',
            '.main-content', '.post-content', '.entry-content'
        ]

        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                return content_elem.get_text(strip=True)

        # é»˜è®¤æå–bodyå†…å®¹
        body = soup.body
        return body.get_text(strip=True) if body else ""

    def _extract_meta_data(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """æå–å…ƒæ•°æ®"""
        meta = {}

        # æå–metaæ ‡ç­¾
        for meta_tag in soup.find_all('meta'):
            name = meta_tag.get('name') or meta_tag.get('property')
            content = meta_tag.get('content')
            if name and content:
                meta[name] = content

        # æå–Open Graphæ•°æ®
        og_data = {}
        for og_tag in soup.find_all('meta', property=re.compile(r'^og:')):
            og_data[og_tag['property']] = og_tag.get('content', '')

        meta['open_graph'] = og_data

        return meta

    def _extract_structured_data(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """æå–ç»“æ„åŒ–æ•°æ®"""
        structured_data = {}

        # æå–JSON-LDæ•°æ®
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_ld_scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, list):
                    structured_data['json_ld'] = data
                else:
                    structured_data.setdefault('json_ld', []).append(data)
            except:
                continue

        # æå–å¾®æ•°æ®
        microdata = []
        for item in soup.find_all(attrs={'itemtype': True}):
            item_data = {
                'type': item.get('itemtype'),
                'properties': {}
            }
            for prop in item.find_all(attrs={'itemprop': True}):
                item_data['properties'][prop.get('itemprop')] = prop.get_text(strip=True)
            microdata.append(item_data)

        structured_data['microdata'] = microdata

        return structured_data

    def _is_content_page(self, url: str, a_tag) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå†…å®¹é¡µé¢"""
        text = a_tag.get_text(strip=True).lower()
        href = url.lower()

        # å†…å®¹é¡µé¢ç‰¹å¾
        content_keywords = ['æ–‡ç« ', 'æ–°é—»', 'åšå®¢', 'post', 'article', 'news', 'blog']
        skip_keywords = ['ç™»å½•', 'æ³¨å†Œ', 'å¹¿å‘Š', 'about', 'contact', 'privacy']

        return (any(keyword in text or keyword in href for keyword in content_keywords) and
                not any(keyword in text or keyword in href for keyword in skip_keywords))

    def _is_api_endpoint(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºAPIç«¯ç‚¹"""
        parsed = urlparse(url)
        path = parsed.path.lower()

        # APIç‰¹å¾
        api_patterns = ['/api/', '/v1/', '/v2/', '/rest/', '/graphql']

        return any(pattern in path for pattern in api_patterns)

    async def _process_text_content(self, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """AIå¤„ç†æ–‡æœ¬å†…å®¹"""
        if not self.ai_coordinator:
            return {"summary": "AIæœªå¯ç”¨"}

        try:
            content = page_data.get("content", "")
            if not content:
                return {"summary": "æ— å†…å®¹"}

            prompt = f"è¯·åˆ†æä»¥ä¸‹ç½‘é¡µå†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š\n\n{content[:2000]}"

            result = await self.ai_coordinator.run('analyze_content', content=prompt)

            if result.get('status') == 'success':
                return result.get('result', {})
            else:
                return {"summary": "AIåˆ†æå¤±è´¥"}

        except Exception as e:
            logger.error(f"æ–‡æœ¬å†…å®¹å¤„ç†å¤±è´¥: {e}")
            return {"error": str(e)}

    async def _analyze_page_structure(self, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """AIåˆ†æé¡µé¢ç»“æ„"""
        if not self.ai_coordinator:
            return {"structure": "AIæœªå¯ç”¨"}

        try:
            # ä½¿ç”¨AIåˆ†æé¡µé¢ç»“æ„
            structure_info = {
                "url": page_data.get("url"),
                "title": page_data.get("title"),
                "has_structured_data": bool(page_data.get("structured_data")),
                "content_length": len(page_data.get("content", ""))
            }

            prompt = f"åˆ†æé¡µé¢ç»“æ„ï¼š{json.dumps(structure_info, ensure_ascii=False)}"

            result = await self.ai_coordinator.run('analyze_content', content=prompt)

            if result.get('status') == 'success':
                return result.get('result', {})
            else:
                return {"structure": "AIåˆ†æå¤±è´¥"}

        except Exception as e:
            logger.error(f"é¡µé¢ç»“æ„åˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}

    async def _analyze_api_structure(self, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """AIåˆ†æAPIç»“æ„"""
        if not self.ai_coordinator:
            return {"api": "AIæœªå¯ç”¨"}

        try:
            json_data = page_data.get("json_data")
            if not json_data:
                return {"api": "éAPIå“åº”"}

            prompt = f"åˆ†æAPIå“åº”ç»“æ„ï¼š\n{json.dumps(json_data, ensure_ascii=False, indent=2)[:2000]}"

            result = await self.ai_coordinator.run('analyze_content', content=prompt)

            if result.get('status') == 'success':
                return result.get('result', {})
            else:
                return {"api": "AIåˆ†æå¤±è´¥"}

        except Exception as e:
            logger.error(f"APIç»“æ„åˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}

    async def _ai_post_analysis(self, data_collected: List[Dict], target_data: str) -> Dict[str, Any]:
        """AIååˆ†æï¼šæ•°æ®æ±‡æ€»å’Œæ´å¯Ÿ"""
        if not self.ai_coordinator:
            return {"analysis": "AIæœªå¯ç”¨"}

        try:
            # æ•°æ®æ±‡æ€»
            summary = {
                "total_pages": len(data_collected),
                "content_types": {},
                "data_quality": {},
                "patterns": []
            }

            for item in data_collected:
                content_type = item.get("content_type", "unknown")
                summary["content_types"][content_type] = summary["content_types"].get(content_type, 0) + 1

            # AIæ·±åº¦åˆ†æ
            analysis_prompt = f"""
            åˆ†æçˆ¬å–æ•°æ®æ±‡æ€»ï¼š
            - æ€»é¡µé¢æ•°: {summary['total_pages']}
            - å†…å®¹ç±»å‹åˆ†å¸ƒ: {summary['content_types']}
            - ç›®æ ‡æ•°æ®ç±»å‹: {target_data}

            è¯·æä¾›ï¼š
            1. æ•°æ®è´¨é‡è¯„ä¼°
            2. å‘ç°çš„æ¨¡å¼å’Œè¶‹åŠ¿
            3. æ½œåœ¨çš„ä»·å€¼æ´å¯Ÿ
            4. è¿›ä¸€æ­¥åˆ†æå»ºè®®
            """

            result = await self.ai_coordinator.run('complex_reasoning', content=analysis_prompt)

            if result.get('status') == 'success':
                summary.update(result.get('result', {}))
                return summary
            else:
                return {"analysis": "AIåˆ†æå¤±è´¥", "basic_stats": summary}

        except Exception as e:
            logger.error(f"AIååˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}

    async def _reverse_engineering_analysis(self, data_collected: List[Dict]) -> Dict[str, Any]:
        """é€†å‘å·¥ç¨‹åˆ†æ"""
        if not self.ai_coordinator:
            return {"reverse_engineering": "AIæœªå¯ç”¨"}

        try:
            analysis = {
                "patterns_discovered": [],
                "potential_apis": [],
                "data_structures": [],
                "security_insights": [],
                "optimization_opportunities": []
            }

            # åˆ†ææ•°æ®æ¨¡å¼
            for item in data_collected:
                if item.get("structured_data"):
                    analysis["data_structures"].append({
                        "url": item["url"],
                        "structures": list(item["structured_data"].keys())
                    })

                # æŸ¥æ‰¾APIæ¨¡å¼
                if "api" in item.get("url", "").lower():
                    analysis["potential_apis"].append(item["url"])

            # AIé€†å‘æ¨ç†
            reverse_prompt = f"""
            åŸºäºä»¥ä¸‹æ•°æ®è¿›è¡Œé€†å‘å·¥ç¨‹åˆ†æï¼š
            æ•°æ®ç»“æ„: {analysis['data_structures'][:5]}
            æ½œåœ¨API: {analysis['potential_apis'][:5]}

            è¯·æ¨æ–­ï¼š
            1. ç³»ç»Ÿæ¶æ„æ¨¡å¼
            2. æ•°æ®æµå‘
            3. æ½œåœ¨çš„å®‰å…¨æ¼æ´
            4. æ€§èƒ½ä¼˜åŒ–ç‚¹
            5. æ‰©å±•å¯èƒ½æ€§
            """

            result = await self.ai_coordinator.run('complex_reasoning', content=reverse_prompt)

            if result.get('status') == 'success':
                analysis.update(result.get('result', {}))
                return analysis
            else:
                return {"basic_analysis": analysis}

        except Exception as e:
            logger.error(f"é€†å‘å·¥ç¨‹åˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}

    async def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        if not self.ai_coordinator:
            return ["å¯ç”¨AIåŠŸèƒ½ä»¥è·å¾—æ›´è¯¦ç»†çš„å»ºè®®"]

        try:
            context = {
                "pages_crawled": results.get("pages_crawled", 0),
                "data_types": list(set(item.get("content_type", "unknown") for item in results.get("data_collected", []))),
                "ai_analysis": bool(results.get("ai_analysis")),
                "reverse_engineering": bool(results.get("reverse_engineering"))
            }

            prompt = f"""
            åŸºäºçˆ¬è™«æ‰§è¡Œç»“æœç”Ÿæˆä¼˜åŒ–å»ºè®®ï¼š
            {json.dumps(context, ensure_ascii=False, indent=2)}

            è¯·æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®ï¼ŒåŒ…æ‹¬ï¼š
            1. çˆ¬å–ç­–ç•¥ä¼˜åŒ–
            2. æ•°æ®å¤„ç†æ”¹è¿›
            3. AIåŠŸèƒ½å¢å¼º
            4. æ€§èƒ½ä¼˜åŒ–
            5. æ‰©å±•å»ºè®®
            """

            result = await self.ai_coordinator.run('task_planning', content=prompt)

            if result.get('status') == 'success':
                recommendations = result.get('result', {}).get('recommendations', [])
                return recommendations if isinstance(recommendations, list) else [str(recommendations)]
            else:
                return ["AIå»ºè®®ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥AIåè°ƒå™¨çŠ¶æ€"]

        except Exception as e:
            logger.error(f"ç”Ÿæˆå»ºè®®å¤±è´¥: {e}")
            return [f"å»ºè®®ç”Ÿæˆå¼‚å¸¸: {str(e)}"]

    async def post_run(self, result: Dict[str, Any]) -> None:
        """æ¸…ç†èµ„æº"""
        await super().post_run(result)

        if self.session:
            await self.session.close()

        if self.ai_coordinator:
            # å¯é€‰ï¼šæ¸…ç†AIåè°ƒå™¨èµ„æº
            pass

    async def on_error(self, error: Exception) -> None:
        """é”™è¯¯å¤„ç†"""
        await super().on_error(error)

        if self.session:
            await self.session.close()
