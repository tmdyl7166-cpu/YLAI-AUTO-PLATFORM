#!/usr/bin/env python3
"""
AIåŠŸèƒ½é«˜çº§é…ç½®ä¼˜åŒ–ç³»ç»Ÿ
è‡ªåŠ¨ä¼˜åŒ–AIæ¨¡å‹é…ç½®ã€å‚æ•°è°ƒä¼˜å’Œæ€§èƒ½æå‡
"""

import asyncio
import json
import time
import logging
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, asdict
from datetime import datetime
import aiohttp
from pathlib import Path
import yaml

from backend.core.base import BaseScript
from backend.core.registry import registry
from backend.scripts.ai_coordinator import AIModelCoordinator


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    model_name: str
    base_url: str
    port: int
    parameters: Dict[str, Any]
    performance_metrics: Dict[str, Any]
    optimization_history: List[Dict[str, Any]]
    last_optimized: float
    status: str  # 'active', 'optimizing', 'error'

    def __post_init__(self):
        if self.parameters is None:
            self.parameters = {}
        if self.performance_metrics is None:
            self.performance_metrics = {}
        if self.optimization_history is None:
            self.optimization_history = []


@dataclass
class OptimizationTask:
    """ä¼˜åŒ–ä»»åŠ¡"""
    task_id: str
    target_model: str
    optimization_type: str  # 'parameter_tuning', 'config_optimization', 'performance_boost'
    current_config: Dict[str, Any]
    proposed_config: Dict[str, Any]
    expected_improvement: Dict[str, Any]
    status: str  # 'pending', 'running', 'completed', 'failed'
    created_at: float
    completed_at: Optional[float] = None
    results: Optional[Dict[str, Any]] = None

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = time.time()


@registry.register("ai_optimizer")
class AIOptimizerScript(BaseScript):
    """AIåŠŸèƒ½é«˜çº§é…ç½®ä¼˜åŒ–ç³»ç»Ÿ"""

    name = "ai_optimizer"
    description = "AIåŠŸèƒ½é«˜çº§é…ç½®ä¼˜åŒ–ç³»ç»Ÿ"
    version = "2.0.0"

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        # AIåè°ƒå™¨
        self.ai_coordinator = None

        # æ¨¡å‹é…ç½®
        self.model_configs: Dict[str, ModelConfig] = {}

        # ä¼˜åŒ–ä»»åŠ¡
        self.optimization_tasks: List[OptimizationTask] = []

        # é…ç½®
        self.config = {
            'optimization_interval': 3600,  # 1å°æ—¶ä¼˜åŒ–ä¸€æ¬¡
            'benchmark_duration': 300,     # 5åˆ†é’ŸåŸºå‡†æµ‹è¯•
            'max_concurrent_optimizations': 2,
            'auto_rollback': True,
            'performance_threshold': 0.05,  # 5%æ€§èƒ½æå‡é˜ˆå€¼
            'config_backup_path': 'backend/data/ai_config_backups',
        }

        # é»˜è®¤æ¨¡å‹é…ç½®
        self.default_configs = {
            'qwen3:8b': {
                'temperature': 0.7,
                'top_p': 0.9,
                'max_tokens': 4096,
                'repetition_penalty': 1.1,
                'context_window': 8192
            },
            'llama3.1:8b': {
                'temperature': 0.8,
                'top_p': 0.95,
                'max_tokens': 4096,
                'repetition_penalty': 1.15,
                'context_window': 8192
            },
            'deepseek-r1:8b': {
                'temperature': 0.6,
                'top_p': 0.85,
                'max_tokens': 8192,
                'repetition_penalty': 1.05,
                'context_window': 16384
            },
            'gpt-oss:20b': {
                'temperature': 0.9,
                'top_p': 0.98,
                'max_tokens': 4096,
                'repetition_penalty': 1.2,
                'context_window': 8192
            }
        }

        # HTTPå®¢æˆ·ç«¯
        self.session = None

    async def pre_run(self, **kwargs):
        """åˆå§‹åŒ–"""
        await super().pre_run(**kwargs)

        # åˆå§‹åŒ–HTTPå®¢æˆ·ç«¯
        self.session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30))

        # åˆå§‹åŒ–AIåè°ƒå™¨
        try:
            self.ai_coordinator = AIModelCoordinator()
            await self.ai_coordinator.initialize()
            self.logger.info("âœ… AIåè°ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ AIåè°ƒå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.ai_coordinator = None

        # åˆå§‹åŒ–æ¨¡å‹é…ç½®
        await self._initialize_model_configs()

    async def run(self, action: str, **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œä¼˜åŒ–æ“ä½œ
        """
        try:
            if action == 'optimize_model':
                result = await self._optimize_model(**kwargs)
            elif action == 'benchmark_models':
                result = await self._benchmark_models(**kwargs)
            elif action == 'auto_tune':
                result = await self._auto_tune(**kwargs)
            elif action == 'performance_analysis':
                result = await self._performance_analysis(**kwargs)
            elif action == 'config_backup':
                result = await self._config_backup(**kwargs)
            else:
                result = {"status": "error", "error": f"æœªçŸ¥æ“ä½œ: {action}"}

            return result

        except Exception as e:
            self.logger.error(f"ä¼˜åŒ–æ“ä½œå¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}

    async def _initialize_model_configs(self):
        """åˆå§‹åŒ–æ¨¡å‹é…ç½®"""
        for model_name, default_config in self.default_configs.items():
            port_map = {
                'qwen3:8b': 11434,
                'llama3.1:8b': 11435,
                'deepseek-r1:8b': 11436,
                'gpt-oss:20b': 11437
            }

            config = ModelConfig(
                model_name=model_name,
                base_url='http://localhost',
                port=port_map.get(model_name, 11434),
                parameters=default_config.copy(),
                performance_metrics={},
                optimization_history=[],
                status='active'
            )

            self.model_configs[model_name] = config

        self.logger.info(f"âœ… åˆå§‹åŒ–äº† {len(self.model_configs)} ä¸ªæ¨¡å‹é…ç½®")

    async def _optimize_model(self, model_name: str, **kwargs) -> Dict[str, Any]:
        """ä¼˜åŒ–å•ä¸ªæ¨¡å‹"""
        if model_name not in self.model_configs:
            return {"status": "error", "error": f"æ¨¡å‹ {model_name} ä¸å­˜åœ¨"}

        try:
            config = self.model_configs[model_name]

            # åˆ›å»ºä¼˜åŒ–ä»»åŠ¡
            task = OptimizationTask(
                task_id=f"opt_{int(time.time())}_{hash(model_name) % 10000}",
                target_model=model_name,
                optimization_type=kwargs.get('optimization_type', 'parameter_tuning'),
                current_config=config.parameters.copy(),
                proposed_config={},
                expected_improvement={}
            )

            self.optimization_tasks.append(task)

            # æ‰§è¡Œä¼˜åŒ–
            result = await self._execute_optimization(task)

            return result

        except Exception as e:
            self.logger.error(f"æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}

    async def _execute_optimization(self, task: OptimizationTask) -> Dict[str, Any]:
        """æ‰§è¡Œä¼˜åŒ–ä»»åŠ¡"""
        try:
            task.status = 'running'
            config = self.model_configs[task.target_model]

            # AIç”Ÿæˆä¼˜åŒ–å»ºè®®
            if self.ai_coordinator:
                optimization_suggestions = await self._ai_generate_optimization_suggestions(task)
                task.proposed_config = optimization_suggestions.get('proposed_config', {})
                task.expected_improvement = optimization_suggestions.get('expected_improvement', {})

            # åº”ç”¨ä¼˜åŒ–é…ç½®
            if task.proposed_config:
                # å¤‡ä»½å½“å‰é…ç½®
                await self._backup_config(config)

                # åº”ç”¨æ–°é…ç½®
                config.parameters.update(task.proposed_config)
                config.last_optimized = time.time()

                # åŸºå‡†æµ‹è¯•
                benchmark_result = await self._benchmark_model_config(config)

                # è¯„ä¼°ä¼˜åŒ–æ•ˆæœ
                improvement = self._evaluate_optimization_improvement(
                    task.current_config, task.proposed_config, benchmark_result
                )

                task.results = {
                    'benchmark_result': benchmark_result,
                    'improvement': improvement,
                    'applied_config': task.proposed_config.copy()
                }

                # è®°å½•ä¼˜åŒ–å†å²
                config.optimization_history.append({
                    'timestamp': time.time(),
                    'task_id': task.task_id,
                    'type': task.optimization_type,
                    'old_config': task.current_config,
                    'new_config': task.proposed_config,
                    'improvement': improvement,
                    'benchmark': benchmark_result
                })

                # å¦‚æœä¼˜åŒ–æ•ˆæœä¸ä½³ï¼Œå›æ»šé…ç½®
                if self.config['auto_rollback'] and improvement.get('overall_score', 0) < 0:
                    await self._rollback_config(config)
                    task.results['rolled_back'] = True

            task.status = 'completed'
            task.completed_at = time.time()

            return {
                "status": "success",
                "task_id": task.task_id,
                "model": task.target_model,
                "optimization_type": task.optimization_type,
                "improvement": task.results.get('improvement', {}),
                "applied": not task.results.get('rolled_back', False)
            }

        except Exception as e:
            task.status = 'failed'
            task.results = {"error": str(e)}
            self.logger.error(f"ä¼˜åŒ–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}

    async def _ai_generate_optimization_suggestions(self, task: OptimizationTask) -> Dict[str, Any]:
        """AIç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        if not self.ai_coordinator:
            return {}

        try:
            config = self.model_configs[task.target_model]

            optimization_prompt = f"""
            ä¸ºAIæ¨¡å‹ç”Ÿæˆä¼˜åŒ–å»ºè®®ï¼š
            æ¨¡å‹: {task.target_model}
            å½“å‰é…ç½®: {json.dumps(task.current_config, ensure_ascii=False, indent=2)}
            ä¼˜åŒ–ç±»å‹: {task.optimization_type}
            å½“å‰æ€§èƒ½: {json.dumps(config.performance_metrics, ensure_ascii=False, indent=2)}

            è¯·æä¾›ï¼š
            1. å‚æ•°ä¼˜åŒ–å»ºè®®
            2. é¢„æœŸæ€§èƒ½æå‡
            3. é…ç½®è°ƒæ•´ç†ç”±
            4. æ½œåœ¨é£é™©è¯„ä¼°
            5. å›æ»šå»ºè®®
            """

            result = await self.ai_coordinator.run('task_planning', content=optimization_prompt)

            if result.get('status') == 'success':
                suggestions = result.get('result', {})

                # è§£æå»ºè®®
                proposed_config = {}
                expected_improvement = {}

                # æå–é…ç½®å»ºè®®
                if 'parameter_suggestions' in suggestions:
                    for param, value in suggestions['parameter_suggestions'].items():
                        if isinstance(value, (int, float)):
                            proposed_config[param] = value

                # æå–é¢„æœŸæ”¹è¿›
                if 'expected_improvements' in suggestions:
                    expected_improvement = suggestions['expected_improvements']

                return {
                    'proposed_config': proposed_config,
                    'expected_improvement': expected_improvement,
                    'reasoning': suggestions.get('reasoning', ''),
                    'risks': suggestions.get('risks', [])
                }

            return {}

        except Exception as e:
            self.logger.error(f"AIä¼˜åŒ–å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
            return {}

    async def _benchmark_model_config(self, config: ModelConfig) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•æ¨¡å‹é…ç½®"""
        try:
            # å‡†å¤‡æµ‹è¯•æç¤º
            test_prompts = [
                "è¯·è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹",
                "åˆ†æå½“å‰ç§‘æŠ€è¡Œä¸šçš„è¶‹åŠ¿",
                "æè¿°ä¸€ä¸ªåˆ›æ–°çš„å•†ä¸šæ¨¡å¼",
                "è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸç†"
            ]

            results = []
            total_response_time = 0
            total_tokens = 0

            for prompt in test_prompts:
                try:
                    start_time = time.time()

                    # å‘é€è¯·æ±‚åˆ°æ¨¡å‹
                    url = f"{config.base_url}:{config.port}/api/generate"
                    payload = {
                        "model": config.model_name,
                        "prompt": prompt,
                        "stream": False,
                        **config.parameters
                    }

                    async with self.session.post(url, json=payload) as response:
                        if response.status == 200:
                            data = await response.json()
                            response_time = time.time() - start_time

                            result = {
                                'prompt': prompt,
                                'response_time': response_time,
                                'success': True,
                                'response_length': len(data.get('response', '')),
                                'tokens_generated': data.get('eval_count', 0)
                            }

                            total_response_time += response_time
                            total_tokens += result['tokens_generated']
                        else:
                            result = {
                                'prompt': prompt,
                                'response_time': time.time() - start_time,
                                'success': False,
                                'error': f"HTTP {response.status}"
                            }

                    results.append(result)

                except Exception as e:
                    results.append({
                        'prompt': prompt,
                        'success': False,
                        'error': str(e)
                    })

            # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
            successful_requests = sum(1 for r in results if r['success'])
            avg_response_time = total_response_time / len(results) if results else 0
            success_rate = successful_requests / len(results) if results else 0
            avg_tokens_per_second = total_tokens / total_response_time if total_response_time > 0 else 0

            benchmark_result = {
                'total_requests': len(results),
                'successful_requests': successful_requests,
                'success_rate': success_rate,
                'avg_response_time': avg_response_time,
                'avg_tokens_per_second': avg_tokens_per_second,
                'total_tokens': total_tokens,
                'detailed_results': results
            }

            # æ›´æ–°é…ç½®çš„æ€§èƒ½æŒ‡æ ‡
            config.performance_metrics = {
                'last_benchmark': time.time(),
                'avg_response_time': avg_response_time,
                'success_rate': success_rate,
                'tokens_per_second': avg_tokens_per_second
            }

            return benchmark_result

        except Exception as e:
            self.logger.error(f"åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return {"error": str(e), "success": False}

    def _evaluate_optimization_improvement(self, old_config: Dict, new_config: Dict,
                                          benchmark_result: Dict) -> Dict[str, Any]:
        """è¯„ä¼°ä¼˜åŒ–æ”¹è¿›"""
        try:
            improvement = {
                'response_time_improvement': 0.0,
                'success_rate_improvement': 0.0,
                'throughput_improvement': 0.0,
                'overall_score': 0.0
            }

            # è¿™é‡Œåº”è¯¥æ¯”è¾ƒæ–°æ—§é…ç½®çš„æ€§èƒ½å·®å¼‚
            # ç”±äºæ²¡æœ‰å†å²åŸºå‡†æ•°æ®ï¼Œè¿™é‡Œä½¿ç”¨ç®€å•çš„è¯„ä¼°

            success_rate = benchmark_result.get('success_rate', 0)
            avg_response_time = benchmark_result.get('avg_response_time', 0)

            # åŸºäºå½“å‰æ€§èƒ½è®¡ç®—åˆ†æ•°
            response_time_score = max(0, 1 - avg_response_time / 10)  # 10ç§’ä»¥å†…å¾—æ»¡åˆ†
            success_rate_score = success_rate
            throughput_score = benchmark_result.get('avg_tokens_per_second', 0) / 100  # æ ‡å‡†åŒ–

            improvement['overall_score'] = (response_time_score + success_rate_score + throughput_score) / 3

            return improvement

        except Exception as e:
            self.logger.error(f"ä¼˜åŒ–æ”¹è¿›è¯„ä¼°å¤±è´¥: {e}")
            return {'overall_score': 0.0, 'error': str(e)}

    async def _benchmark_models(self, **kwargs) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•æ‰€æœ‰æ¨¡å‹"""
        try:
            results = {}

            for model_name, config in self.model_configs.items():
                self.logger.info(f"ğŸ”¬ åŸºå‡†æµ‹è¯•æ¨¡å‹: {model_name}")
                benchmark = await self._benchmark_model_config(config)
                results[model_name] = benchmark

            # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
            comparison = await self._generate_benchmark_comparison(results)

            return {
                "status": "success",
                "benchmark_results": results,
                "comparison": comparison
            }

        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}

    async def _generate_benchmark_comparison(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æ¯”è¾ƒæŠ¥å‘Š"""
        if not self.ai_coordinator:
            return {"comparison": "AIæœªå¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆè¯¦ç»†æ¯”è¾ƒ"}

        try:
            comparison_prompt = f"""
            åˆ†ææ¨¡å‹åŸºå‡†æµ‹è¯•ç»“æœæ¯”è¾ƒï¼š
            æµ‹è¯•ç»“æœ: {json.dumps(results, ensure_ascii=False, indent=2)}

            è¯·æä¾›ï¼š
            1. æ€§èƒ½æ’å
            2. å„æ¨¡å‹ä¼˜ç¼ºç‚¹
            3. ä½¿ç”¨å»ºè®®
            4. ä¼˜åŒ–å»ºè®®
            """

            result = await self.ai_coordinator.run('complex_reasoning', content=comparison_prompt)

            if result.get('status') == 'success':
                return result.get('result', {})
            else:
                return {"comparison": "AIåˆ†æå¤±è´¥"}

        except Exception as e:
            self.logger.error(f"åŸºå‡†æµ‹è¯•æ¯”è¾ƒç”Ÿæˆå¤±è´¥: {e}")
            return {"error": str(e)}

    async def _auto_tune(self, **kwargs) -> Dict[str, Any]:
        """è‡ªåŠ¨è°ƒä¼˜æ‰€æœ‰æ¨¡å‹"""
        try:
            tuning_results = {}

            for model_name in self.model_configs.keys():
                self.logger.info(f"ğŸ›ï¸ è‡ªåŠ¨è°ƒä¼˜æ¨¡å‹: {model_name}")

                # æ‰§è¡Œä¼˜åŒ–
                result = await self._optimize_model(
                    model_name,
                    optimization_type='auto_tune'
                )

                tuning_results[model_name] = result

            # ç”Ÿæˆè°ƒä¼˜æ€»ç»“
            summary = await self._generate_tuning_summary(tuning_results)

            return {
                "status": "success",
                "tuning_results": tuning_results,
                "summary": summary
            }

        except Exception as e:
            self.logger.error(f"è‡ªåŠ¨è°ƒä¼˜å¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}

    async def _generate_tuning_summary(self, tuning_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆè°ƒä¼˜æ€»ç»“"""
        if not self.ai_coordinator:
            return {"summary": "AIæœªå¯ç”¨"}

        try:
            summary_prompt = f"""
            åˆ†æè‡ªåŠ¨è°ƒä¼˜ç»“æœæ€»ç»“ï¼š
            è°ƒä¼˜ç»“æœ: {json.dumps(tuning_results, ensure_ascii=False, indent=2)}

            è¯·æä¾›ï¼š
            1. æ•´ä½“ä¼˜åŒ–æ•ˆæœè¯„ä¼°
            2. å„æ¨¡å‹æ”¹è¿›æƒ…å†µ
            3. æœ€ä½³é…ç½®æ¨è
            4. åç»­ä¼˜åŒ–å»ºè®®
            """

            result = await self.ai_coordinator.run('complex_reasoning', content=summary_prompt)

            if result.get('status') == 'success':
                return result.get('result', {})
            else:
                return {"summary": "AIæ€»ç»“ç”Ÿæˆå¤±è´¥"}

        except Exception as e:
            self.logger.error(f"è°ƒä¼˜æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
            return {"error": str(e)}

    async def _performance_analysis(self, **kwargs) -> Dict[str, Any]:
        """æ€§èƒ½åˆ†æ"""
        try:
            analysis = {
                'model_performance': {},
                'optimization_history': {},
                'trends': {},
                'recommendations': []
            }

            # æ”¶é›†å„æ¨¡å‹æ€§èƒ½æ•°æ®
            for model_name, config in self.model_configs.items():
                analysis['model_performance'][model_name] = {
                    'current_config': config.parameters,
                    'performance_metrics': config.performance_metrics,
                    'optimization_count': len(config.optimization_history),
                    'last_optimized': config.last_optimized
                }

                analysis['optimization_history'][model_name] = config.optimization_history[-5:]  # æœ€è¿‘5æ¬¡

            # AIåˆ†ææ€§èƒ½è¶‹åŠ¿
            if self.ai_coordinator:
                trends_analysis = await self._ai_analyze_performance_trends(analysis)
                analysis['trends'] = trends_analysis

                # ç”Ÿæˆå»ºè®®
                recommendations = await self._ai_generate_performance_recommendations(analysis)
                analysis['recommendations'] = recommendations

            return {
                "status": "success",
                "performance_analysis": analysis
            }

        except Exception as e:
            self.logger.error(f"æ€§èƒ½åˆ†æå¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}

    async def _ai_analyze_performance_trends(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """AIåˆ†ææ€§èƒ½è¶‹åŠ¿"""
        if not self.ai_coordinator:
            return {}

        try:
            trends_prompt = f"""
            åˆ†æAIæ¨¡å‹æ€§èƒ½è¶‹åŠ¿ï¼š
            æ€§èƒ½æ•°æ®: {json.dumps(analysis, ensure_ascii=False, indent=2)}

            è¯·åˆ†æï¼š
            1. æ€§èƒ½å˜åŒ–è¶‹åŠ¿
            2. ä¼˜åŒ–æ•ˆæœè¯„ä¼°
            3. ç“¶é¢ˆè¯†åˆ«
            4. æœªæ¥é¢„æµ‹
            """

            result = await self.ai_coordinator.run('complex_reasoning', content=trends_prompt)

            if result.get('status') == 'success':
                return result.get('result', {})
            else:
                return {}

        except Exception as e:
            self.logger.error(f"æ€§èƒ½è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            return {}

    async def _ai_generate_performance_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """AIç”Ÿæˆæ€§èƒ½å»ºè®®"""
        if not self.ai_coordinator:
            return ["å¯ç”¨AIåè°ƒå™¨ä»¥è·å¾—è¯¦ç»†å»ºè®®"]

        try:
            rec_prompt = f"""
            åŸºäºæ€§èƒ½åˆ†æç”Ÿæˆä¼˜åŒ–å»ºè®®ï¼š
            åˆ†ææ•°æ®: {json.dumps(analysis, ensure_ascii=False, indent=2)}

            è¯·æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®åˆ—è¡¨ã€‚
            """

            result = await self.ai_coordinator.run('task_planning', content=rec_prompt)

            if result.get('status') == 'success':
                recommendations = result.get('result', {}).get('recommendations', [])
                return recommendations if isinstance(recommendations, list) else [str(recommendations)]
            else:
                return ["AIå»ºè®®ç”Ÿæˆå¤±è´¥"]

        except Exception as e:
            self.logger.error(f"æ€§èƒ½å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
            return [f"å»ºè®®ç”Ÿæˆå¼‚å¸¸: {str(e)}"]

    async def _backup_config(self, config: ModelConfig):
        """å¤‡ä»½é…ç½®"""
        try:
            backup_path = Path(self.config['config_backup_path'])
            backup_path.mkdir(parents=True, exist_ok=True)

            backup_file = backup_path / f"{config.model_name}_{int(time.time())}.json"

            backup_data = {
                'timestamp': time.time(),
                'model_name': config.model_name,
                'parameters': config.parameters.copy(),
                'performance_metrics': config.performance_metrics.copy()
            }

            with open(backup_file, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)

            self.logger.info(f"âœ… é…ç½®å¤‡ä»½å®Œæˆ: {backup_file}")

        except Exception as e:
            self.logger.error(f"é…ç½®å¤‡ä»½å¤±è´¥: {e}")

    async def _rollback_config(self, config: ModelConfig):
        """å›æ»šé…ç½®"""
        try:
            backup_path = Path(self.config['config_backup_path'])
            if not backup_path.exists():
                return

            # æ‰¾åˆ°æœ€æ–°çš„å¤‡ä»½
            backup_files = list(backup_path.glob(f"{config.model_name}_*.json"))
            if not backup_files:
                return

            latest_backup = max(backup_files, key=lambda x: x.stat().st_mtime)

            with open(latest_backup, 'r', encoding='utf-8') as f:
                backup_data = json.load(f)

            # æ¢å¤é…ç½®
            config.parameters = backup_data.get('parameters', config.parameters)

            self.logger.info(f"âœ… é…ç½®å›æ»šå®Œæˆ: {latest_backup}")

        except Exception as e:
            self.logger.error(f"é…ç½®å›æ»šå¤±è´¥: {e}")

    async def _config_backup(self, **kwargs) -> Dict[str, Any]:
        """æ‰‹åŠ¨é…ç½®å¤‡ä»½"""
        try:
            backed_up = []

            for config in self.model_configs.values():
                await self._backup_config(config)
                backed_up.append(config.model_name)

            return {
                "status": "success",
                "backed_up_models": backed_up,
                "backup_path": self.config['config_backup_path']
            }

        except Exception as e:
            self.logger.error(f"é…ç½®å¤‡ä»½å¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}

    async def post_run(self, result: Dict[str, Any]) -> None:
        """åå¤„ç†"""
        await super().post_run(result)

        if self.session:
            await self.session.close()

        self.logger.info("âš™ï¸ AIé…ç½®ä¼˜åŒ–ç³»ç»Ÿå·²åœæ­¢")