import asyncio
import time
from typing import List, Dict, Any

from backend.core.task import Task, Node
try:
    from backend.core.metrics import (
        PIPELINE_NODE_SECONDS,
        PIPELINE_NODE_FAILURES,
        PIPELINE_RUNS_OVERALL,
    )  # type: ignore
except Exception:
    class _No:
        def labels(self, *_, **__):
            return self
        def observe(self, *_):
            pass
        def inc(self, *_):
            pass
    PIPELINE_NODE_SECONDS = PIPELINE_NODE_FAILURES = PIPELINE_RUNS_OVERALL = _No()
from backend.core.logger import logger


class Pipeline:
    """
    å¤šæ­¥éª¤ä»»åŠ¡é¡ºåºæµæ°´çº¿æ‰§è¡Œå™¨ï¼ˆä¸²è¡Œï¼‰
    ä½¿ç”¨æ–¹å¼ï¼š
      p = Pipeline(kernel)
      p.add_task(Task(script="demo", params={...}))
      result = p.run()
    """

    def __init__(self, kernel):
        self.kernel = kernel
        self.tasks: List[Task] = []

    def add_task(self, task: Task):
        self.tasks.append(task)

    def run(self) -> Dict[str, Any]:
        logger.info("ğŸš€ å¯åŠ¨ä»»åŠ¡æµæ°´çº¿ï¼ˆä¸²è¡Œï¼‰")
        result_cache = None

        for index, task in enumerate(self.tasks, start=1):
            logger.info(f"â¡ï¸ æ‰§è¡Œç¬¬ {index} æ­¥ï¼š{task.script}")

            try:
                # æŠŠä¸Šä¸€ä¸ªä»»åŠ¡ç»“æœä¼ ç»™ä¸‹ä¸€ä¸ª
                params = dict(task.params or {})
                if result_cache is not None:
                    params["prev_result"] = result_cache

                result_cache = self.kernel.run(task.script, **params)

            except Exception as e:
                logger.error(f"âŒ æµæ°´çº¿ä¸­æ–­äºæ­¥éª¤ {index}: {str(e)}")
                return {
                    "status": "failed",
                    "step": index,
                    "error": str(e)
                }

        logger.info("âœ… æµæ°´çº¿å…¨éƒ¨å®Œæˆ")
        return {
            "status": "success",
            "result": result_cache
        }


class DAGPipeline:
    """
    DAG å¹¶è¡Œæµæ°´çº¿å¼•æ“ï¼ˆåŸºäº asyncioï¼‰
    ä½¿ç”¨æ–¹å¼:
      pipeline = DAGPipeline(kernel)
      await pipeline.run(nodes, max_concurrency=4, node_timeout=300)
    """

    def __init__(self, kernel, max_concurrency: int = 4):
        self.kernel = kernel
        self.max_concurrency = max_concurrency

    def validate(self, nodes: List[Node], registered_scripts: List[str]) -> Dict[str, Any]:
        """
        é™æ€æ ¡éªŒï¼š
         - id å”¯ä¸€
         - script å·²æ³¨å†Œ
         - depends_on ä¸­å¼•ç”¨å­˜åœ¨
         - æ— ç¯æ£€æµ‹ï¼ˆæ‹“æ‰‘æ£€æŸ¥ï¼‰
        è¿”å› { ok: bool, errors: [str] }
        """
        errors = []
        ids = [n.id for n in nodes]
        if len(ids) != len(set(ids)):
            errors.append("èŠ‚ç‚¹ id ä¸å”¯ä¸€")

        id_set = set(ids)
        for n in nodes:
            if n.script not in registered_scripts:
                errors.append(f"èŠ‚ç‚¹ {n.id} ä½¿ç”¨äº†æœªæ³¨å†Œè„šæœ¬: {n.script}")
            for dep in n.depends_on:
                if dep not in id_set:
                    errors.append(f"èŠ‚ç‚¹ {n.id} ä¾èµ–äº†ä¸å­˜åœ¨çš„èŠ‚ç‚¹: {dep}")

        # æ— ç¯æ£€æµ‹: Kahn æ‹“æ‰‘æ³•
        indeg = {nid: 0 for nid in ids}
        graph = {nid: [] for nid in ids}
        for n in nodes:
            for d in n.depends_on:
                graph[d].append(n.id)
                indeg[n.id] += 1

        q = [nid for nid, d in indeg.items() if d == 0]
        visited = 0
        while q:
            cur = q.pop()
            visited += 1
            for nei in graph[cur]:
                indeg[nei] -= 1
                if indeg[nei] == 0:
                    q.append(nei)
        if visited != len(ids):
            errors.append("å­˜åœ¨ç¯ä¾èµ–æˆ–ä¾èµ–æ— æ³•è§£æï¼ˆé DAGï¼‰")

        return {"ok": len(errors) == 0, "errors": errors}

    async def run(self, nodes: List[Node], max_concurrency: int = None, node_timeout: int = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œ DAGï¼š
        - nodes: Node åˆ—è¡¨ï¼ˆä»»æ„é¡ºåºï¼‰
        - max_concurrency: é™åˆ¶å¹¶å‘ï¼ˆNone åˆ™ä½¿ç”¨ self.max_concurrencyï¼‰
        - node_timeout: å•èŠ‚ç‚¹è¶…æ—¶ï¼ˆç§’ï¼‰ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶
        è¿”å›ï¼š
        {
          "status": "success"|"failed",
          "nodes": {
             "<node_id>": {"status": "success"/"failed"/"skipped"/"cancelled", "result": ..., "error": ..., "duration": float}
          },
          "order": [å®Œæˆé¡ºåºçš„ node_id åˆ—è¡¨]
        }
        """
        if max_concurrency is None:
            max_concurrency = self.max_concurrency

        # internal maps
        id_to_node = {n.id: n for n in nodes}
        dependents = {n.id: [] for n in nodes}
        remaining_deps = {n.id: len(n.depends_on) for n in nodes}

        for n in nodes:
            for dep in n.depends_on:
                dependents[dep].append(n.id)

        semaphore = asyncio.Semaphore(max_concurrency)
        loop = asyncio.get_event_loop()

        results: Dict[str, Dict[str, Any]] = {}
        completed_order: List[str] = []
        running_tasks: Dict[str, asyncio.Task] = {}

        # upstream aggregation helper
        def gather_upstream_results(node_id: str) -> Dict[str, Any]:
            # collect results of dependencies: {dep_id: results[dep_id]["result"]}
            data: Dict[str, Any] = {}
            for dep in id_to_node[node_id].depends_on:
                # include full result object to be flexible
                data[dep] = results.get(dep, {}).get("result")
            return data

        def eval_condition(expr: Any, context: Dict[str, Any]) -> bool:
            if expr in (None, "", True):
                return True
            if expr is False:
                return False
            try:
                local_ctx = {**context, "null": None, "None": None, "true": True, "false": False}
                return bool(eval(str(expr), {"__builtins__": {}}, local_ctx))
            except Exception:
                return True

        async def exec_node(node_id: str):
            node = id_to_node[node_id]
            await semaphore.acquire()
            start = time.time()
            logger.info(f"å¼€å§‹æ‰§è¡ŒèŠ‚ç‚¹ {node_id} -> è„šæœ¬ {node.script}")
            try:
                # prepare params copy and inject upstream results
                params = dict(node.params or {})
                params["_upstream_results"] = gather_upstream_results(node_id)

                # æ¡ä»¶åˆ¤å®šï¼šä¸æ»¡è¶³åˆ™è·³è¿‡
                cond_ok = eval_condition(getattr(node, "condition", None), {"up": params["_upstream_results"], "params": params})
                if not cond_ok:
                    duration = 0.0
                    results[node_id] = {"status": "skipped", "result": None, "error": None, "duration": duration}
                    logger.info(f"èŠ‚ç‚¹ {node_id} æ¡ä»¶ä¸æ»¡è¶³ï¼Œè·³è¿‡æ‰§è¡Œ")
                else:
                    # ç¼“å­˜å‘½ä¸­åˆ™ç›´æ¥è¿”å›
                    cached = self.kernel.try_cache(node.script, params)
                    if cached is not None:
                        duration = time.time() - start
                        results[node_id] = {"status": "success", "result": cached, "error": None, "duration": duration, "cached": True}
                        logger.info(f"èŠ‚ç‚¹ {node_id} ç¼“å­˜å‘½ä¸­ (t={duration:.2f}s)")
                        try:
                            PIPELINE_NODE_SECONDS.labels(mode="async", script=node.script).observe(duration)
                        except Exception:
                            pass
                    else:
                    # run kernel.run in threadpool (kernel.run æ˜¯åŒæ­¥)
                        coro = loop.run_in_executor(None, self.kernel.run, node.script, **params)
                        if node_timeout:
                            res = await asyncio.wait_for(coro, timeout=node_timeout)
                        else:
                            res = await coro

                        duration = time.time() - start
                        results[node_id] = {"status": "success", "result": res, "error": None, "duration": duration}
                        self.kernel.save_cache(node.script, params, res)
                        logger.info(f"èŠ‚ç‚¹ {node_id} æ‰§è¡ŒæˆåŠŸ (t={duration:.2f}s)")
                        try:
                            PIPELINE_NODE_SECONDS.labels(mode="async", script=node.script).observe(duration)
                        except Exception:
                            pass

            except asyncio.CancelledError:
                duration = time.time() - start
                results[node_id] = {"status": "cancelled", "result": None, "error": "cancelled", "duration": duration}
                logger.error(f"èŠ‚ç‚¹ {node_id} è¢«å–æ¶ˆ")
            except Exception as e:
                duration = time.time() - start
                results[node_id] = {"status": "failed", "result": None, "error": str(e), "duration": duration}
                logger.error(f"èŠ‚ç‚¹ {node_id} æ‰§è¡Œå¤±è´¥: {e}")
                try:
                    PIPELINE_NODE_FAILURES.labels(mode="async", script=node.script).inc()
                except Exception:
                    pass
            finally:
                semaphore.release()

            # mark completed
            completed_order.append(node_id)

            # schedule dependents if ready
            for depn in dependents.get(node_id, []):
                remaining_deps[depn] -= 1
                # if any predecessor failed, we mark dependent as blocked (do not schedule)
                blocked = False
                for p in id_to_node[depn].depends_on:
                    if results.get(p, {}).get("status") == "failed":
                        blocked = True
                        break
                if blocked:
                    # mark this node as blocked/skipped
                    results[depn] = {"status": "skipped", "result": None, "error": f"ä¾èµ–èŠ‚ç‚¹å¤±è´¥ï¼ŒèŠ‚ç‚¹ {depn} è¢«è·³è¿‡", "duration": 0.0}
                    completed_order.append(depn)
                    # propagate skip to its dependents too
                    for nextn in dependents.get(depn, []):
                        remaining_deps[nextn] -= 1
                    continue

                if remaining_deps[depn] == 0 and depn not in running_tasks and results.get(depn) is None:
                    # schedule execution
                    running_tasks[depn] = asyncio.create_task(exec_node(depn))

        # initially schedule all nodes with remaining_deps == 0
        for nid, cnt in list(remaining_deps.items()):
            if cnt == 0:
                running_tasks[nid] = asyncio.create_task(exec_node(nid))

        # wait for all running tasks to complete
        if running_tasks:
            await asyncio.gather(*running_tasks.values())

        # determine overall status
        overall = "success"
        for _nid, r in results.items():
            if r["status"] == "failed":
                overall = "failed"
                break

        # record overall (async) run status
        try:
            PIPELINE_RUNS_OVERALL.labels(mode="async", status=overall).inc()
        except Exception:
            pass

        return {
            "status": overall,
            "nodes": results,
            "order": completed_order
        }
 
