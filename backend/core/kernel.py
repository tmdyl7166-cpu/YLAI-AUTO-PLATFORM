import asyncio
import importlib
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import json
import requests
import hashlib
from .registry import registry
from .logger import logger
from backend.core.pipeline import Pipeline
from backend.core.task import Task
from backend.core.policy import GlobalPolicy
import importlib
import time

def execute_node(node, ws_send=None):
    """
    node: {id, script, params, depends_on}
    ws_send: WebSocket å‘é€å‡½æ•°ï¼Œå¯é€‰
    """
    node_id = node["id"]
    script_name = node["script"]
    params = node.get("params", {})
    
    try:
        module = importlib.import_module(f"scripts.spider.{script_name}")
        start_time = time.time()
        
        # æ¨¡æ‹Ÿé€ç§’çŠ¶æ€æ¨é€
        for sec in range(1, 4):
            if ws_send:
                ws_send(node_id, {"status":"running", "elapsed":sec})
            time.sleep(1)
        
        result = module.run(params)
        node["status"] = result.get("status", "success")
        node["data"] = result.get("data", None)
        
        if ws_send:
            ws_send(node_id, {"status": node["status"], "elapsed": time.time() - start_time})
        
    except Exception as e:
        node["status"] = "failed"
        node["error"] = str(e)
        if ws_send:
            ws_send(node_id, {"status":"failed","error":str(e)})


class Kernel:
    def __init__(self):
        self.registry = registry
        logger.info("ğŸš€ åç«¯å†…æ ¸åˆå§‹åŒ–...")
        self._cache: dict[str, dict] = {}

    def load_scripts(self):
        self.registry.auto_register("backend.scripts")
        logger.info(f"âœ… å·²æ³¨å†Œè„šæœ¬: {', '.join(self.registry.list_all())}")

    def run(self, name: str, **kwargs):
        logger.info(f"â–¶ï¸ å¯åŠ¨è„šæœ¬: {name}")
        # å…¨å±€ç­–ç•¥æ¥å…¥ï¼šæ ¹æ®ç­‰çº§è°ƒæ•´å‚æ•°ä¸å®‰å…¨è¡Œä¸º
        level = GlobalPolicy.level()
        params = dict(kwargs)
        if level == 0:
            # SAFEï¼šä¸¥æ ¼é€Ÿç‡ä¸ç¦ç”¨ä»£ç†/AI ä¿®å¤
            params["delay"] = max(params.get("delay", 0), GlobalPolicy.request_interval_ms() / 1000.0)
            params["use_proxy"] = False
            params["_ai_fix"] = False
        elif level >= 2:
            # STRESS/RESEARCHï¼šæå‡å¹¶å‘ä¸Šé™ï¼ˆç”±è„šæœ¬è‡ªè¡Œæ¶ˆè´¹ï¼‰
            params["concurrency"] = GlobalPolicy.max_concurrency()
        if not GlobalPolicy.allow_ai_fix():
            params["_ai_fix"] = False
        script = self.registry.get(name)
        return script.run(**params)

    async def run_async(self, name: str, **kwargs):
        """
        å¼‚æ­¥å°è£…ï¼šåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥çš„ runï¼Œä¾¿äºå¹¶å‘åœºæ™¯ä½¿ç”¨ã€‚
        ç”¨äº DAG å¹¶å‘å¼•æ“æˆ–éœ€è¦ asyncio å…¼å®¹æ—¶ã€‚
        """
        return await asyncio.to_thread(self.run, name, **kwargs)

    def list_scripts(self):
        return self.registry.list_all()

    # === AI è‡ªåŠ¨å‚æ•°ç”Ÿæˆï¼ˆæœ€å°å®ç°ï¼‰ ===
    def ai_generate_params(self, node_id: str, error_msg: str, task_state: dict, base_params: dict | None = None) -> dict:
        """
        åŸºäºå¤±è´¥ä¿¡æ¯ä¸ä¸Šä¸‹æ–‡ï¼Œè®©æœ¬åœ° AI ç”Ÿæˆæ–°çš„å‚æ•°ã€‚
        - ä½¿ç”¨å­—æ®µç™½åå•ï¼šä»…å…è®¸ä¿®æ”¹ base_params ä¸­å·²å­˜åœ¨çš„é”®ã€‚
        - å®‰å…¨è¿”å›ï¼šè‹¥ AI å“åº”ä¸å¯è§£ææˆ–ä¸åˆç†ï¼Œåˆ™è¿”å›ç©ºå­—å…¸ã€‚
        """
        base = os.environ.get("OLLAMA_URL", os.environ.get("AI_URL", "http://127.0.0.1:11434")).rstrip("/")
        ai_url = f"{base}/api/generate"
        model = os.environ.get("AI_MODEL", "deepseek-r1:8b")
        base_params = dict(base_params or {})

        # å‹ç¼©ä¸Šä¸‹æ–‡ï¼Œé¿å…è¿‡å¤§
        context = {
            "node_id": node_id,
            "error": str(error_msg)[:2000],
            "base_params": base_params,
        }
        prompt = (
            "ä½ æ˜¯å‚æ•°è°ƒä¼˜åŠ©æ‰‹ã€‚\n"
            "æ ¹æ®é”™è¯¯ä¿¡æ¯ä¸å½“å‰å‚æ•°ï¼Œç»™å‡ºä¸€ä¸ªä»…åŒ…å«éœ€è¦ä¿®æ”¹é”®çš„ JSON å¯¹è±¡ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "- åªè¾“å‡º JSONï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚\n"
            "- ä»…åŒ…å«å…è®¸å˜æ›´çš„å­—æ®µï¼ˆä¸ base_params åŒåé”®ï¼‰ã€‚\n"
            "- ä¸è¦æ–°å¢ç»“æ„æ€§å­—æ®µã€‚\n"
            f"ä¸Šä¸‹æ–‡: {json.dumps(context, ensure_ascii=False)}\n"
            "è¾“å‡ºç¤ºä¾‹ï¼š{\"timeout\": 30, \"retry\": 1}"
        )

        try:
            resp = requests.post(
                ai_url,
                json={"model": model, "prompt": prompt, "stream": False},
                timeout=30,
            )
            resp.raise_for_status()
            data = resp.json()
            text = (data.get("response") or "").strip()
            # å…¼å®¹æ€ç»´é“¾ï¼Œæå– JSON ç‰‡æ®µ
            start = text.find("{")
            end = text.rfind("}")
            if start != -1 and end != -1 and end > start:
                text = text[start:end+1]
            suggested = json.loads(text)
            if not isinstance(suggested, dict):
                return {}
            # ç™½åå•ï¼šä»…å…è®¸ä¿®æ”¹ base_params é‡Œå­˜åœ¨çš„é”®
            cleaned = {}
            for k, v in suggested.items():
                if k in base_params:
                    # åŸºç¡€ç±»å‹é™åˆ¶ï¼Œé¿å…å¯æ‰§è¡Œå¯¹è±¡
                    if isinstance(v, (str, int, float, bool)) or v is None or isinstance(v, (list, dict)):
                        cleaned[k] = v
            return cleaned
        except Exception as e:
            logger.error(f"AI ç”Ÿæˆå‚æ•°å¤±è´¥: {e}")
            return {}

    # === å†…å­˜ç¼“å­˜ï¼ˆæœ€å°å®ç°ï¼‰ ===
    @staticmethod
    def _hash_params(script: str, params: dict) -> str:
        try:
            dump = json.dumps({"script": script, "params": params}, ensure_ascii=False, sort_keys=True)
        except Exception:
            dump = f"{script}:{str(params)}"
        return hashlib.sha256(dump.encode("utf-8")).hexdigest()

    def try_cache(self, script: str, params: dict) -> dict | None:
        # å¯é€šè¿‡ params._cache = False å…³é—­ç¼“å­˜
        if params.get("_cache") is False:
            return None
        key = self._hash_params(script, {k: v for k, v in params.items() if not k.startswith("_")})
        return self._cache.get(key)

    def save_cache(self, script: str, params: dict, result: dict):
        if params.get("_cache") is False:
            return
        key = self._hash_params(script, {k: v for k, v in params.items() if not k.startswith("_")})
        self._cache[key] = result
    def run_pipeline(self, task_list: list):
        pipeline = Pipeline(self)

        for item in task_list:
            task = Task(
                script=item["script"],
                params=item.get("params", {})
            )
            pipeline.add_task(task)

        return pipeline.run()


# ===== ç®€æ˜“ DAG æ‰§è¡Œï¼ˆç¤ºä¾‹ç‰ˆï¼‰ =====

def execute_node(node: dict, ws_send=None):
    node_id = node["id"]
    script_name = node["script"]
    params = node.get("params", {})
    category = node.get("category", "spider")  # spider/ai/process

    try:
        module = importlib.import_module(f"backend.scripts.{category}.{script_name}")
        start_time = time.time()
        if ws_send:
            ws_send(node_id, {"status": "running", "elapsed": 0})
        result = module.run(params)
        node["status"] = result.get("status", "success")
        node["data"] = result.get("data", None)
        if ws_send:
            ws_send(node_id, {"status": node["status"], "elapsed": time.time() - start_time})
    except Exception as e:
        node["status"] = "failed"
        node["error"] = str(e)
        if ws_send:
            ws_send(node_id, {"status": "failed", "error": str(e)})


def run_pipeline(nodes: list[dict], max_workers: int = 4, ws_send=None):
    """
    ç®€æ˜“å¹¶è¡Œ DAG æ‰§è¡Œï¼ˆä¸å¤„ç†å¾ªç¯ä¾èµ–ï¼‰ï¼Œåœ¨å¯è¿è¡Œæ—¶è°ƒåº¦èŠ‚ç‚¹åˆ°çº¿ç¨‹æ± ã€‚
    """
    # è·Ÿè¸ªä¾èµ–ä¸çŠ¶æ€
    depends = {n["id"]: set(n.get("depends_on", [])) for n in nodes}
    node_map = {n["id"]: n for n in nodes}

    executor = ThreadPoolExecutor(max_workers=max_workers)
    in_progress = {}
    finished = set()

    def submit_ready():
        for nid, deps in list(depends.items()):
            if nid in finished or nid in in_progress:
                continue
            if not deps:  # æ— ä¾èµ–ï¼Œæˆ–ä¾èµ–å·²å®Œæˆ
                if ws_send:
                    ws_send(nid, {"status": "queued"})
                future = executor.submit(execute_node, node_map[nid], ws_send)
                in_progress[nid] = future

    # åˆå§‹æ ‡è®°ç­‰å¾…çŠ¶æ€
    for nid, d in depends.items():
        if d and ws_send:
            ws_send(nid, {"status": "waiting"})

    submit_ready()

    while len(finished) < len(nodes):
        done = [nid for nid, fut in in_progress.items() if fut.done()]
        for nid in done:
            _ = in_progress.pop(nid)
            finished.add(nid)
            # è§£é™¤å…¶ä»–èŠ‚ç‚¹å¯¹å®ƒçš„ä¾èµ–
            for dn in depends.values():
                dn.discard(nid)
        if done:
            submit_ready()
        else:
            # é¿å…ç©ºè½¬å ç”¨ CPU
            time.sleep(0.05)

    executor.shutdown(wait=True)
    return nodes
